{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import initializers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "\n",
    "fcc_elements = [\"Ag\", \"Al\", \"Au\", \"Cu\", \"Ir\", \"Ni\", \"Pb\", \"Pd\", \"Pt\", \"Rh\", \"Th\", \"Yb\"]\n",
    "bcc_elements = [\"Ba\", \"Cr\", \"Cs\", \"Eu\", \"Fe\", \"Li\", \"Mn\", \"Mo\", \"Na\", \"Nb\", \"Rb\", \"Ta\", \"V\", \"W\" ]\n",
    "hcp_elements = [\"Be\", \"Ca\", \"Cd\", \"Co\", \"Dy\", \"Er\", \"Gd\", \"Hf\", \"Ho\", \"Lu\", \"Mg\", \"Re\", \n",
    "                \"Ru\", \"Sc\", \"Tb\", \"Ti\", \"Tl\", \"Tm\", \"Y\", \"Zn\", \"Zr\"]\n",
    "others = [\"Si\", \"Ge\"] # \"Si\" and \"Ge\" are Face-centered diamond-cubic;\n",
    "\n",
    "elements = fcc_elements + others + bcc_elements + hcp_elements\n",
    "\n",
    "querable_mendeleev = [\"atomic_number\", \"atomic_volume\", \"boiling_point\", \"en_ghosh\",  \"evaporation_heat\", \"heat_of_formation\",\n",
    "                     \"lattice_constant\", \"melting_point\", \"specific_heat\"]\n",
    "querable_pymatgen = [\"atomic_mass\", \"atomic_radius\", \"electrical_resistivity\",\"molar_volume\", \"bulk_modulus\", \"youngs_modulus\",\n",
    "                     \"average_ionic_radius\", \"density_of_solid\", \"coefficient_of_linear_thermal_expansion\"]\n",
    "querable_values = querable_mendeleev + querable_pymatgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "\n",
    "with open(\"all_values.csv\", \"r\") as f:\n",
    "    all_values = json.load(f)\n",
    "\n",
    "# Pandas Dataframe\n",
    "df = pd.DataFrame(all_values, columns=querable_values)\n",
    "\n",
    "# We will patch some of the values that are not available in the datasets.\n",
    "\n",
    "# Value for the CTE of Cesium\n",
    "index_Cs = df.index[df['atomic_number'] == 55]\n",
    "df.iloc[index_Cs, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000097 \n",
    "# Value from: David R. Lide (ed), CRC Handbook of Chemistry and Physics, 84th Edition. CRC Press. Boca Raton, Florida, 2003\n",
    "\n",
    "# Value for the CTE of Rubidium\n",
    "index_Rb = df.index[df['atomic_number'] == 37]\n",
    "df.iloc[index_Rb, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000090 \n",
    "# Value from: https://www.azom.com/article.aspx?ArticleID=1834\n",
    "\n",
    "# Value for the Evaporation Heat of Ruthenium\n",
    "index_Ru = df.index[df['atomic_number'] == 44]\n",
    "df.iloc[index_Ru, df.columns.get_loc(\"evaporation_heat\")] = 595 # kJ/mol \n",
    "# Value from: https://www.webelements.com/ruthenium/thermochemistry.html\n",
    "\n",
    "# Value for the Bulk Modulus of Zirconium\n",
    "index_Zr = df.index[df['atomic_number'] == 40]\n",
    "df.iloc[index_Zr, df.columns.get_loc(\"bulk_modulus\")] = 94 # GPa \n",
    "# Value from: https://materialsproject.org/materials/mp-131/\n",
    "\n",
    "# Value for the Bulk Modulus of Germanium\n",
    "index_Ge = df.index[df['atomic_number'] == 32]\n",
    "df.iloc[index_Ge, df.columns.get_loc(\"bulk_modulus\")] = 77.2 # GPa \n",
    "# Value from: https://www.crystran.co.uk/optical-materials/germanium-ge\n",
    "\n",
    "# Value for the Young's Modulus of Germanium\n",
    "index_Ge = df.index[df['atomic_number'] == 32]\n",
    "df.iloc[index_Ge, df.columns.get_loc(\"youngs_modulus\")] = 102.7 # GPa \n",
    "# Value from: https://www.crystran.co.uk/optical-materials/germanium-ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atomic_number</th>\n",
       "      <th>atomic_volume</th>\n",
       "      <th>boiling_point</th>\n",
       "      <th>en_ghosh</th>\n",
       "      <th>evaporation_heat</th>\n",
       "      <th>heat_of_formation</th>\n",
       "      <th>lattice_constant</th>\n",
       "      <th>melting_point</th>\n",
       "      <th>specific_heat</th>\n",
       "      <th>atomic_mass</th>\n",
       "      <th>atomic_radius</th>\n",
       "      <th>electrical_resistivity</th>\n",
       "      <th>molar_volume</th>\n",
       "      <th>bulk_modulus</th>\n",
       "      <th>average_ionic_radius</th>\n",
       "      <th>density_of_solid</th>\n",
       "      <th>coefficient_of_linear_thermal_expansion</th>\n",
       "      <th>youngs_modulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>10.30</td>\n",
       "      <td>2485.0</td>\n",
       "      <td>0.147217</td>\n",
       "      <td>254.1</td>\n",
       "      <td>284.9</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1235.10</td>\n",
       "      <td>0.237</td>\n",
       "      <td>107.868200</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.630000e-08</td>\n",
       "      <td>10.27</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.086667</td>\n",
       "      <td>10490.0</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2740.0</td>\n",
       "      <td>0.150078</td>\n",
       "      <td>284.1</td>\n",
       "      <td>330.9</td>\n",
       "      <td>4.05</td>\n",
       "      <td>933.50</td>\n",
       "      <td>0.900</td>\n",
       "      <td>26.981539</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.700000e-08</td>\n",
       "      <td>10.00</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>10.20</td>\n",
       "      <td>3080.0</td>\n",
       "      <td>0.261370</td>\n",
       "      <td>340.0</td>\n",
       "      <td>368.2</td>\n",
       "      <td>4.08</td>\n",
       "      <td>1337.58</td>\n",
       "      <td>0.129</td>\n",
       "      <td>196.966569</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.200000e-08</td>\n",
       "      <td>10.21</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>19300.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>7.10</td>\n",
       "      <td>2840.0</td>\n",
       "      <td>0.151172</td>\n",
       "      <td>304.6</td>\n",
       "      <td>337.4</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1356.60</td>\n",
       "      <td>0.385</td>\n",
       "      <td>63.546000</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.720000e-08</td>\n",
       "      <td>7.11</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>8920.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "      <td>8.54</td>\n",
       "      <td>4403.0</td>\n",
       "      <td>0.251060</td>\n",
       "      <td>604.0</td>\n",
       "      <td>669.0</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2683.00</td>\n",
       "      <td>0.133</td>\n",
       "      <td>192.217000</td>\n",
       "      <td>1.35</td>\n",
       "      <td>4.700000e-08</td>\n",
       "      <td>8.52</td>\n",
       "      <td>320.0</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>22650.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>528.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   atomic_number  atomic_volume  boiling_point  en_ghosh  evaporation_heat  \\\n",
       "0             47          10.30         2485.0  0.147217             254.1   \n",
       "1             13          10.00         2740.0  0.150078             284.1   \n",
       "2             79          10.20         3080.0  0.261370             340.0   \n",
       "3             29           7.10         2840.0  0.151172             304.6   \n",
       "4             77           8.54         4403.0  0.251060             604.0   \n",
       "\n",
       "   heat_of_formation  lattice_constant  melting_point  specific_heat  \\\n",
       "0              284.9              4.09        1235.10          0.237   \n",
       "1              330.9              4.05         933.50          0.900   \n",
       "2              368.2              4.08        1337.58          0.129   \n",
       "3              337.4              3.61        1356.60          0.385   \n",
       "4              669.0              3.84        2683.00          0.133   \n",
       "\n",
       "   atomic_mass  atomic_radius  electrical_resistivity  molar_volume  \\\n",
       "0   107.868200           1.60            1.630000e-08         10.27   \n",
       "1    26.981539           1.25            2.700000e-08         10.00   \n",
       "2   196.966569           1.35            2.200000e-08         10.21   \n",
       "3    63.546000           1.35            1.720000e-08          7.11   \n",
       "4   192.217000           1.35            4.700000e-08          8.52   \n",
       "\n",
       "   bulk_modulus  average_ionic_radius  density_of_solid  \\\n",
       "0         100.0              1.086667           10490.0   \n",
       "1          76.0              0.675000            2700.0   \n",
       "2         220.0              1.070000           19300.0   \n",
       "3         140.0              0.820000            8920.0   \n",
       "4         320.0              0.765000           22650.0   \n",
       "\n",
       "   coefficient_of_linear_thermal_expansion  youngs_modulus  \n",
       "0                                 0.000019            83.0  \n",
       "1                                 0.000023            70.0  \n",
       "2                                 0.000014            78.0  \n",
       "3                                 0.000017           130.0  \n",
       "4                                 0.000006           528.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we'll create the heatmap again\n",
    "all_labels = df['youngs_modulus'].tolist()\n",
    "\n",
    "# make a list of all the inputs\n",
    "all_inputs = df.values.tolist()\n",
    "\n",
    "# Make a list of the young's modulus column so that we can append it to the end\n",
    "youngs_modulus = list(df['youngs_modulus'])\n",
    "\n",
    "# Drop young's modulus column\n",
    "df = df.drop('youngs_modulus', axis = 1)\n",
    "\n",
    "# Create a new young's modulus column, this time at the end\n",
    "df[\"youngs_modulus\"] = youngs_modulus\n",
    "\n",
    "# create a list of all the labels\n",
    "labels = df.columns.tolist()\n",
    "\n",
    "# Check that it's at the end\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I will make lists of all the variables I would like to optmize\n",
    "# The loop can then be modified to loop through any of these lists\n",
    "\n",
    "# These are the values for the validation split\n",
    "# The original value was 0.10\n",
    "val_list = [0.12, 0.11, 0.10, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04]\n",
    "\n",
    "# These are the values that will be tested for the rms learning rate\n",
    "# The original values used was 0.002\n",
    "rms_list = [0.005, 0.003, 0.0025, 0.002, 0.0015, 0.001, 0.0005, 0.0002, 0.00001, 0.000001]\n",
    "\n",
    "# These are the values that will be tested for the number of nodes for the first layer\n",
    "# The original value was 32\n",
    "nodes_1 = list(range(10,33))\n",
    "\n",
    "# These are the values that will be tested for the number of nodes for the second layer\n",
    "# The original value was 64\n",
    "nodes_2 = list(range(10,75))\n",
    "\n",
    "# I will also test how many layers is optimal, but this doesn't require a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Nodes (Layer 1) Test\n",
    "\n",
    "# How long the loop will be for the nodes\n",
    "node_end = 5\n",
    "\n",
    "# Create an empty list to which we'll add our final values\n",
    "# This should be an alternating list consisting of the variable(s) dropped and the mae value\n",
    "node1_values = []\n",
    "\n",
    "# Open a file where will write our output to\n",
    "v_file = open(\"test_node1_data.csv\", \"w\")\n",
    "\n",
    "# Counting variables\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 01663: early stopping\n",
      "The current iteration is \n",
      " 0\n",
      "10 8.41 52.704\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00240: early stopping\n",
      "The current iteration is \n",
      " 1\n",
      "11 87.849 66.022\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00223: early stopping\n",
      "The current iteration is \n",
      " 2\n",
      "12 81.294 52.696\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 01260: early stopping\n",
      "The current iteration is \n",
      " 3\n",
      "13 38.752 108.528\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00214: early stopping\n",
      "The current iteration is \n",
      " 4\n",
      "14 96.024 33.325\n"
     ]
    }
   ],
   "source": [
    "# Nodes (Layer 1) Test\n",
    "while i < node_end:\n",
    "        # open file\n",
    "        v_file = open(\"test_node1_data.csv\", \"a\")\n",
    "   \n",
    "        # Reassign the dataframe\n",
    "        sf = df\n",
    "    \n",
    "        # Drop youngs modulus\n",
    "        sf = df.drop('youngs_modulus', axis = 1)\n",
    "\n",
    "        all_values = [list(sf.iloc[x]) for x in range(len(all_values))]\n",
    "\n",
    "        # SETS\n",
    "\n",
    "        # List of lists are turned into Numpy arrays to facilitate calculations in steps to follow (Normalization).\n",
    "        all_values = np.array(all_values, dtype = float) \n",
    "        #print(\"Shape of Values:\", all_values.shape)\n",
    "        all_labels = np.array(all_labels, dtype = float)\n",
    "        #print(\"Shape of Labels:\", all_labels.shape)\n",
    "\n",
    "        # Uncomment the line below to shuffle the dataset (we do not do this here to ensure consistent results for every run)\n",
    "        order = np.argsort(np.random.random(all_labels.shape)) # This numpy argsort returns the indexes that would be used to shuffle a list\n",
    "        # order = np.arange(49)\n",
    "        all_values = all_values[order]\n",
    "        all_labels = all_labels[order]\n",
    "\n",
    "        # Training Set\n",
    "        train_labels = all_labels[:44]\n",
    "        train_values = all_values[:44]\n",
    "\n",
    "        # Testing Set\n",
    "        test_labels = all_labels[-5:]\n",
    "        test_values = all_values[-5:]\n",
    "\n",
    "        # NORMALIZATION\n",
    "        mean = np.mean(train_values, axis = 0) # mean\n",
    "        std = np.std(train_values, axis = 0) # standard deviation\n",
    "\n",
    "        train_values = (train_values - mean) / std # input scaling\n",
    "        test_values = (test_values - mean) / std # input scaling\n",
    "\n",
    "        # DEFINITION OF THE MODEL\n",
    "\n",
    "        # The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "        kernel_init = initializers.RandomNormal(seed=0)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(nodes_1[i], activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init))\n",
    "        model.add(Dense(64, activation='relu', kernel_initializer=kernel_init))\n",
    "        model.add(Dense(1, kernel_initializer=kernel_init))\n",
    "\n",
    "        # DEFINITION OF THE OPTIMIZER\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(0.0015) # Root Mean Squared Propagation\n",
    "\n",
    "        # This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "        model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "        # model.summary()\n",
    "\n",
    "        class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "            def on_epoch_end(self, epoch, logs):\n",
    "                sys.stdout.flush()\n",
    "                sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + '\\r') # Updates current Epoch Number\n",
    "\n",
    "        mae_es= keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=10, verbose=1, mode='auto', restore_best_weights=True)    \n",
    "        EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "        # HISTORY Object which contains how the model learned\n",
    "        # Training Values (Properties), Training Labels (Known Young's Moduli)\n",
    "        history = model.fit(train_values, train_labels, batch_size = train_values.shape[0], \n",
    "                        epochs = EPOCHS, verbose = False, validation_split = 0.10, callbacks=[mae_es, PrintEpNum()]) \n",
    "\n",
    "\n",
    "        [loss_train, mae_train] = model.evaluate(train_values, train_labels, verbose=0)\n",
    "        [loss_test, mae_test] = model.evaluate(test_values, test_labels, verbose=0)\n",
    "\n",
    "        # Here is where we append the dropped variable, mae test, and train values\n",
    "        node1_values.append([nodes_1[i],round(mae_train, 3), round(mae_test, 3)])\n",
    "    \n",
    "        # Display the current iteration\n",
    "        print(\"The current iteration is \\n\" ,i)\n",
    "\n",
    "        # write to a file\n",
    "        v_file.write(\" \".join(str(x) for x in node1_values[i]))\n",
    "        v_file.write(\"\\n\")\n",
    "    \n",
    "        # Display the same information being written to the file\n",
    "        print(\" \".join(str(x) for x in node1_values[i]))\n",
    "\n",
    "        # counting variable\n",
    "        i = i + 1\n",
    "    \n",
    "        # Close the file    \n",
    "        v_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without Loop\n",
    "\n",
    "# Create an empty list to which we'll add our final values\n",
    "# This should be an alternating list consisting of the variable(s) dropped and the mae value\n",
    "node1_values = []\n",
    "\n",
    "# Counting variables\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00262: early stopping\n",
      "[[10, 94.895, 96.843]]\n"
     ]
    }
   ],
   "source": [
    "# Another test\n",
    "\n",
    "# Reassign the dataframe\n",
    "sf = df\n",
    "    \n",
    "# Drop youngs modulus\n",
    "sf = df.drop('youngs_modulus', axis = 1)\n",
    "\n",
    "all_values = [list(sf.iloc[x]) for x in range(len(all_values))]\n",
    "\n",
    "# SETS\n",
    "\n",
    "# List of lists are turned into Numpy arrays to facilitate calculations in steps to follow (Normalization).\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "#print(\"Shape of Values:\", all_values.shape)\n",
    "all_labels = np.array(all_labels, dtype = float)\n",
    "#print(\"Shape of Labels:\", all_labels.shape)\n",
    "\n",
    "# Uncomment the line below to shuffle the dataset (we do not do this here to ensure consistent results for every run)\n",
    "order = np.argsort(np.random.random(all_labels.shape)) # This numpy argsort returns the indexes that would be used to shuffle a list\n",
    "# order = np.arange(49)\n",
    "all_values = all_values[order]\n",
    "all_labels = all_labels[order]\n",
    "\n",
    "# Training Set\n",
    "train_labels = all_labels[:44]\n",
    "train_values = all_values[:44]\n",
    "\n",
    "# Testing Set\n",
    "test_labels = all_labels[-5:]\n",
    "test_values = all_values[-5:]\n",
    "\n",
    "# NORMALIZATION\n",
    "mean = np.mean(train_values, axis = 0) # mean\n",
    "std = np.std(train_values, axis = 0) # standard deviation\n",
    "\n",
    "train_values = (train_values - mean) / std # input scaling\n",
    "test_values = (test_values - mean) / std # input scaling\n",
    "\n",
    "# DEFINITION OF THE MODEL\n",
    "\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(nodes_1[i], activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer=kernel_init))\n",
    "model.add(Dense(1, kernel_initializer=kernel_init))\n",
    "\n",
    "# DEFINITION OF THE OPTIMIZER\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(0.0015) # Root Mean Squared Propagation\n",
    "\n",
    "# This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "# model.summary()\n",
    "\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + '\\r') # Updates current Epoch Number\n",
    "\n",
    "mae_es= keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=10, verbose=1, mode='auto', restore_best_weights=True)    \n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# HISTORY Object which contains how the model learned\n",
    "# Training Values (Properties), Training Labels (Known Young's Moduli)\n",
    "history = model.fit(train_values, train_labels, batch_size = train_values.shape[0], \n",
    "                    epochs = EPOCHS, verbose = False, validation_split = 0.10, callbacks=[mae_es, PrintEpNum()]) \n",
    "\n",
    "\n",
    "[loss_train, mae_train] = model.evaluate(train_values, train_labels, verbose=0)\n",
    "[loss_test, mae_test] = model.evaluate(test_values, test_labels, verbose=0)\n",
    "\n",
    "# Here is where we append the dropped variable, mae test, and train values\n",
    "node1_values.append([nodes_1[i],round(mae_train, 3), round(mae_test, 3)])\n",
    "\n",
    "# Display the same information being written to the file\n",
    "print(node1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 01443: early stopping\n",
      "[[10, 43.434, 188.332]]\n"
     ]
    }
   ],
   "source": [
    "# Another test\n",
    "\n",
    "# Reassign the dataframe\n",
    "sf = df\n",
    "    \n",
    "# Drop youngs modulus\n",
    "sf = df.drop('youngs_modulus', axis = 1)\n",
    "\n",
    "all_values = [list(sf.iloc[x]) for x in range(len(all_values))]\n",
    "\n",
    "# SETS\n",
    "\n",
    "# List of lists are turned into Numpy arrays to facilitate calculations in steps to follow (Normalization).\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "#print(\"Shape of Values:\", all_values.shape)\n",
    "all_labels = np.array(all_labels, dtype = float)\n",
    "#print(\"Shape of Labels:\", all_labels.shape)\n",
    "\n",
    "# Uncomment the line below to shuffle the dataset (we do not do this here to ensure consistent results for every run)\n",
    "order = np.argsort(np.random.random(all_labels.shape)) # This numpy argsort returns the indexes that would be used to shuffle a list\n",
    "# order = np.arange(49)\n",
    "all_values = all_values[order]\n",
    "all_labels = all_labels[order]\n",
    "\n",
    "# Training Set\n",
    "train_labels = all_labels[:44]\n",
    "train_values = all_values[:44]\n",
    "\n",
    "# Testing Set\n",
    "test_labels = all_labels[-5:]\n",
    "test_values = all_values[-5:]\n",
    "\n",
    "# NORMALIZATION\n",
    "mean = np.mean(train_values, axis = 0) # mean\n",
    "std = np.std(train_values, axis = 0) # standard deviation\n",
    "\n",
    "train_values = (train_values - mean) / std # input scaling\n",
    "test_values = (test_values - mean) / std # input scaling\n",
    "\n",
    "# DEFINITION OF THE MODEL\n",
    "\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(nodes_1[i], activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer=kernel_init))\n",
    "model.add(Dense(1, kernel_initializer=kernel_init))\n",
    "\n",
    "# DEFINITION OF THE OPTIMIZER\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(0.0015) # Root Mean Squared Propagation\n",
    "\n",
    "# This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "# model.summary()\n",
    "\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + '\\r') # Updates current Epoch Number\n",
    "\n",
    "mae_es= keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=10, verbose=1, mode='auto', restore_best_weights=True)    \n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# HISTORY Object which contains how the model learned\n",
    "# Training Values (Properties), Training Labels (Known Young's Moduli)\n",
    "history = model.fit(train_values, train_labels, batch_size = train_values.shape[0], \n",
    "                    epochs = EPOCHS, verbose = False, validation_split = 0.10, callbacks=[mae_es, PrintEpNum()]) \n",
    "\n",
    "\n",
    "[loss_train, mae_train] = model.evaluate(train_values, train_labels, verbose=0)\n",
    "[loss_test, mae_test] = model.evaluate(test_values, test_labels, verbose=0)\n",
    "\n",
    "# Here is where we append the dropped variable, mae test, and train values\n",
    "node1_values.append([nodes_1[i],round(mae_train, 3), round(mae_test, 3)])\n",
    "\n",
    "# Display the same information being written to the file\n",
    "print(node1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
