{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import initializers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "\n",
    "fcc_elements = [\"Ag\", \"Al\", \"Au\", \"Cu\", \"Ir\", \"Ni\", \"Pb\", \"Pd\", \"Pt\", \"Rh\", \"Th\", \"Yb\"]\n",
    "bcc_elements = [\"Ba\", \"Cr\", \"Cs\", \"Eu\", \"Fe\", \"Li\", \"Mn\", \"Mo\", \"Na\", \"Nb\", \"Rb\", \"Ta\", \"V\", \"W\" ]\n",
    "hcp_elements = [\"Be\", \"Ca\", \"Cd\", \"Co\", \"Dy\", \"Er\", \"Gd\", \"Hf\", \"Ho\", \"Lu\", \"Mg\", \"Re\", \n",
    "                \"Ru\", \"Sc\", \"Tb\", \"Ti\", \"Tl\", \"Tm\", \"Y\", \"Zn\", \"Zr\"]\n",
    "others = [\"Si\", \"Ge\"] # \"Si\" and \"Ge\" are Face-centered diamond-cubic;\n",
    "\n",
    "elements = fcc_elements + others + bcc_elements + hcp_elements\n",
    "\n",
    "querable_mendeleev = [\"atomic_number\", \"atomic_volume\", \"boiling_point\", \"en_ghosh\",  \"evaporation_heat\", \"heat_of_formation\",\n",
    "                     \"lattice_constant\", \"melting_point\", \"specific_heat\"]\n",
    "querable_pymatgen = [\"atomic_mass\", \"atomic_radius\", \"electrical_resistivity\",\"molar_volume\", \"bulk_modulus\", \"youngs_modulus\",\n",
    "                     \"average_ionic_radius\", \"density_of_solid\", \"coefficient_of_linear_thermal_expansion\"]\n",
    "querable_values = querable_mendeleev + querable_pymatgen\n",
    "\n",
    "# Get the data\n",
    "\n",
    "with open(\"all_values.csv\", \"r\") as f:\n",
    "    all_values = json.load(f)\n",
    "\n",
    "# Pandas Dataframe\n",
    "df = pd.DataFrame(all_values, columns=querable_values)\n",
    "\n",
    "# We will patch some of the values that are not available in the datasets.\n",
    "\n",
    "# Value for the CTE of Cesium\n",
    "index_Cs = df.index[df['atomic_number'] == 55]\n",
    "df.iloc[index_Cs, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000097 \n",
    "# Value from: David R. Lide (ed), CRC Handbook of Chemistry and Physics, 84th Edition. CRC Press. Boca Raton, Florida, 2003\n",
    "\n",
    "# Value for the CTE of Rubidium\n",
    "index_Rb = df.index[df['atomic_number'] == 37]\n",
    "df.iloc[index_Rb, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000090 \n",
    "# Value from: https://www.azom.com/article.aspx?ArticleID=1834\n",
    "\n",
    "# Value for the Evaporation Heat of Ruthenium\n",
    "index_Ru = df.index[df['atomic_number'] == 44]\n",
    "df.iloc[index_Ru, df.columns.get_loc(\"evaporation_heat\")] = 595 # kJ/mol \n",
    "# Value from: https://www.webelements.com/ruthenium/thermochemistry.html\n",
    "\n",
    "# Value for the Bulk Modulus of Zirconium\n",
    "index_Zr = df.index[df['atomic_number'] == 40]\n",
    "df.iloc[index_Zr, df.columns.get_loc(\"bulk_modulus\")] = 94 # GPa \n",
    "# Value from: https://materialsproject.org/materials/mp-131/\n",
    "\n",
    "# Value for the Bulk Modulus of Germanium\n",
    "index_Ge = df.index[df['atomic_number'] == 32]\n",
    "df.iloc[index_Ge, df.columns.get_loc(\"bulk_modulus\")] = 77.2 # GPa \n",
    "# Value from: https://www.crystran.co.uk/optical-materials/germanium-ge\n",
    "\n",
    "# Value for the Young's Modulus of Germanium\n",
    "index_Ge = df.index[df['atomic_number'] == 32]\n",
    "df.iloc[index_Ge, df.columns.get_loc(\"youngs_modulus\")] = 102.7 # GPa \n",
    "# Value from: https://www.crystran.co.uk/optical-materials/germanium-ge\n",
    "\n",
    "# First, we'll create the heatmap again\n",
    "all_labels = df['youngs_modulus'].tolist()\n",
    "\n",
    "# make a list of all the inputs\n",
    "all_inputs = df.values.tolist()\n",
    "\n",
    "# Make a list of the young's modulus column so that we can append it to the end\n",
    "youngs_modulus = list(df['youngs_modulus'])\n",
    "\n",
    "# Drop young's modulus column\n",
    "df = df.drop('youngs_modulus', axis = 1)\n",
    "\n",
    "# Create a new young's modulus column, this time at the end\n",
    "df[\"youngs_modulus\"] = youngs_modulus\n",
    "\n",
    "# create a list of all the labels\n",
    "labels = df.columns.tolist()\n",
    "\n",
    "# Check that it's at the end\n",
    "df.head()\n",
    "\n",
    "# Create an 18x18 data frame that displays all the correlation coefficients between the inputs\n",
    "pcorr = df.corr(method = 'pearson')\n",
    "\n",
    "# All values will be between -1 (a negative line implying a 1:1 relationship) and +1 (positive line with 1:1 relationship)\n",
    "# The center diagonal will have all +1 correlations since every variable is correlated with itself\n",
    "# The closer the absolute value is to 1, the more correlated the variables are\n",
    "\n",
    "# This creates a numpy array of the values, which we'll use to make the heatmap\n",
    "p = pcorr.values\n",
    "\n",
    "# The data looks like this\n",
    "pcorr.head()\n",
    "\n",
    "# Now, we're going to make the heatmap\n",
    "fig, ax = plt.subplots(figsize = [11,11])\n",
    "im = ax.imshow(p)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for y in range(p.shape[0]):\n",
    "    for x in range(p.shape[1]):\n",
    "        plt.text(x, y, '%.2f' % p[y, x],\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 color = \"w\",\n",
    "                fontsize = 9)\n",
    "\n",
    "ax.set_title(\"Pearson Correlation Coefficients\")\n",
    "cbar = plt.colorbar(im, shrink = 0.5)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "coeffs = [abs(x) for x in p]\n",
    "#print(coeffs)\n",
    "coeffs = pd.DataFrame(coeffs, columns = labels, index = labels)\n",
    "coeffs.sort_values(by = labels, axis = 0)\n",
    "#print(labels)\n",
    "coeffs.head()\n",
    "\n",
    "# Alright, now we're going to drop one value from pairs that we know are correlated\n",
    "\n",
    "# This will create an 18x18 data frame of T/F values indicating whether the index is >= 0.65\n",
    "ge = coeffs.ge(0.65, axis = 0)\n",
    "\n",
    "# Create an empty list for \"highly correlated\" values\n",
    "hc = []\n",
    "\n",
    "# Create a loop to add the highly correlated values to our list\n",
    "for i in range(0, 18):\n",
    "    for j in range(0, 18):\n",
    "        if i != j:         # We know that each variable is correlated with itself\n",
    "            if ge.iloc[i,j] == True:\n",
    "                hc.append([coeffs.index[i], coeffs.columns[j]])\n",
    "\n",
    "# Now, we have a list of lists consisting of highly correlated variables\n",
    "# Uncomment this below if you want to see it\n",
    "# print(hc)\n",
    "\n",
    "# Let's make a dataframe of this\n",
    "hc_labels = [\"variable_1\", \"variable_2\"]\n",
    "hcdf = pd.DataFrame(hc, columns = hc_labels)\n",
    "hcdf.head(-1)\n",
    "\n",
    "# Looking at the dataframe, we can see that there are repeats, but that's good\n",
    "# We don't have to search for repeats\n",
    "# We can also see that there are only 14 unique variables to be dropped (not including young's modulus)\n",
    "\n",
    "# Remove youngs_modulus from the dataframe\n",
    "hcdf = hcdf[hcdf.variable_1 != \"youngs_modulus\"]\n",
    "hcdf = hcdf[hcdf.variable_2 != \"youngs_modulus\"]\n",
    "\n",
    "# create a list of the variables to be removed\n",
    "to_remove = list()\n",
    "\n",
    "# create a list of the unique variables contained in the first column\n",
    "uniq_vars = hcdf['variable_1'].unique()\n",
    "\n",
    "# Now, we will create a list of all each variable along with its correlated variables\n",
    "# This will be a list of lists, and we will drop a whole list each time\n",
    "# Since they are all associated, there should be no significiant loss in the mae when the associated variables are dropped\n",
    "for i in uniq_vars:\n",
    "    this_list = list(hcdf.loc[hcdf['variable_1'] == i]['variable_2'])\n",
    "    this_list.insert(0, i)\n",
    "    to_remove.append(this_list)\n",
    "        \n",
    "# To see the list of lists, uncomment the line below     \n",
    "print(to_remove)\n",
    "'''\n",
    "# this will add everything in a single list (no list of lists)\n",
    "for i in uniq_vars:\n",
    "    to_remove.append(i)\n",
    "    this_list = list(hcdf.loc[hcdf['variable_1'] == i]['variable_2'])\n",
    "    for x in this_list:\n",
    "        to_remove.append(x)\n",
    "'''\n",
    "# How long the loop will be\n",
    "the_end = len(to_remove)\n",
    "\n",
    "# Create an empty list to which we'll add our final values\n",
    "# This should be an alternating list consisting of the variable(s) dropped and the mae value\n",
    "my_values = []\n",
    "\n",
    "# Open a file where will write our output to\n",
    "v_file = open(\"var_data.csv\", \"w\")\n",
    "\n",
    "# Counting variable\n",
    "i = 0\n",
    "\n",
    "while i < the_end:\n",
    "    # open file\n",
    "    v_file = open(\"var_data.csv\", \"a\")\n",
    "   \n",
    "    # Reassign the dataframe\n",
    "    sf = df\n",
    "    \n",
    "    # Drop youngs modulus\n",
    "    sf = df.drop('youngs_modulus', axis = 1)\n",
    "    \n",
    "    # Now, drop the specified values\n",
    "    for j in range(1, len(to_remove[i])):\n",
    "        sf = sf.drop(to_remove[i][j], axis = 1)\n",
    "\n",
    "\n",
    "    all_values = [list(sf.iloc[x]) for x in range(len(all_values))]\n",
    "\n",
    "    # SETS\n",
    "\n",
    "    # List of lists are turned into Numpy arrays to facilitate calculations in steps to follow (Normalization).\n",
    "    all_values = np.array(all_values, dtype = float) \n",
    "    #print(\"Shape of Values:\", all_values.shape)\n",
    "    all_labels = np.array(all_labels, dtype = float)\n",
    "    #print(\"Shape of Labels:\", all_labels.shape)\n",
    "\n",
    "    # Uncomment the line below to shuffle the dataset (we do not do this here to ensure consistent results for every run)\n",
    "    # order = np.argsort(np.random.random(all_labels.shape)) # This numpy argsort returns the indexes that would be used to shuffle a list\n",
    "    order = np.arange(49)\n",
    "    all_values = all_values[order]\n",
    "    all_labels = all_labels[order]\n",
    "\n",
    "    # Training Set\n",
    "    train_labels = all_labels[:44]\n",
    "    train_values = all_values[:44]\n",
    "\n",
    "    # Testing Set\n",
    "    test_labels = all_labels[-5:]\n",
    "    test_values = all_values[-5:]\n",
    "\n",
    "    # NORMALIZATION\n",
    "    mean = np.mean(train_values, axis = 0) # mean\n",
    "    std = np.std(train_values, axis = 0) # standard deviation\n",
    "\n",
    "    train_values = (train_values - mean) / std # input scaling\n",
    "    test_values = (test_values - mean) / std # input scaling\n",
    "\n",
    "    # DEFINITION OF THE MODEL\n",
    "\n",
    "    # The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "    kernel_init = initializers.RandomNormal(seed=0)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer=kernel_init))\n",
    "    model.add(Dense(1, kernel_initializer=kernel_init))\n",
    "\n",
    "    # DEFINITION OF THE OPTIMIZER\n",
    "\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.002) # Root Mean Squared Propagation\n",
    "\n",
    "    # This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    # model.summary()\n",
    "\n",
    "    class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + '\\r') # Updates current Epoch Number\n",
    "\n",
    "    \n",
    "    mae_es= keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=10, verbose=1, mode='auto', restore_best_weights=True)    \n",
    "    EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "    # HISTORY Object which contains how the model learned\n",
    "    # Training Values (Properties), Training Labels (Known Young's Moduli)\n",
    "    history = model.fit(train_values, train_labels, batch_size = train_values.shape[0], \n",
    "                        epochs = EPOCHS, verbose = False, validation_split=0.1, callbacks=[mae_es, PrintEpNum()]) \n",
    "\n",
    "\n",
    "    [loss_train, mae_train] = model.evaluate(train_values, train_labels, verbose=0)\n",
    "    [loss_test, mae_test] = model.evaluate(test_values, test_labels, verbose=0)\n",
    "\n",
    "    # Here is where we append the dropped variable, mae test, and train values\n",
    "    my_values.append([to_remove[i],round(mae_train, 3), round(mae_test, 3)])\n",
    "    \n",
    "    # Display the current iteration\n",
    "    print(\"The current iteration is \\n\" ,i)\n",
    "\n",
    "    # write to a file\n",
    "    v_file.write(\" \".join(str(x) for x in my_values[i]))\n",
    "    v_file.write(\"\\n\")\n",
    "    \n",
    "    # Display the same information being written to the file\n",
    "    print(\" \".join(str(x) for x in my_values[i]))\n",
    "\n",
    "    # counting variable\n",
    "    i = i + 1\n",
    "    \n",
    "    # Close the file    \n",
    "    v_file.close()\n",
    "    \n",
    "  # Now, we're going to see what the value would be without dropping any variables (besides young's modulus\n",
    "\n",
    "# Reassign the dataframe\n",
    "sf = df\n",
    "    \n",
    "# Drop young's modulus\n",
    "sf = sf.drop('youngs_modulus', axis = 1)\n",
    "\n",
    "all_values = [list(sf.iloc[x]) for x in range(len(all_values))]\n",
    "\n",
    "# SETS\n",
    "\n",
    "# List of lists are turned into Numpy arrays to facilitate calculations in steps to follow (Normalization).\n",
    "all_values = np.array(all_values, dtype = float) \n",
    "#print(\"Shape of Values:\", all_values.shape)\n",
    "all_labels = np.array(all_labels, dtype = float)\n",
    "#print(\"Shape of Labels:\", all_labels.shape)\n",
    "\n",
    "# Uncomment the line below to shuffle the dataset (we do not do this here to ensure consistent results for every run)\n",
    "# order = np.argsort(np.random.random(all_labels.shape)) # This numpy argsort returns the indexes that would be used to shuffle a list\n",
    "order = np.arange(49)\n",
    "all_values = all_values[order]\n",
    "all_labels = all_labels[order]\n",
    "\n",
    "# Training Set\n",
    "train_labels = all_labels[:44]\n",
    "train_values = all_values[:44]\n",
    "\n",
    "# Testing Set\n",
    "test_labels = all_labels[-5:]\n",
    "test_values = all_values[-5:]\n",
    "\n",
    "# NORMALIZATION\n",
    "mean = np.mean(train_values, axis = 0) # mean\n",
    "std = np.std(train_values, axis = 0) # standard deviation\n",
    "\n",
    "train_values = (train_values - mean) / std # input scaling\n",
    "test_values = (test_values - mean) / std # input scaling\n",
    "\n",
    "# DEFINITION OF THE MODEL\n",
    "\n",
    "# The weights of our neural network will be initialized in a random manner, using a seed allows for reproducibility\n",
    "kernel_init = initializers.RandomNormal(seed=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(train_values.shape[1], ), kernel_initializer=kernel_init))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer=kernel_init))\n",
    "model.add(Dense(1, kernel_initializer=kernel_init))\n",
    "\n",
    "# DEFINITION OF THE OPTIMIZER\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(0.002) # Root Mean Squared Propagation\n",
    "\n",
    "# This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "# model.summary()\n",
    "\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + '\\r') # Updates current Epoch Number\n",
    "\n",
    "mae_es= keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=10, verbose=1, mode='auto', restore_best_weights=True)    \n",
    "EPOCHS = 10000 # Number of EPOCHS\n",
    "\n",
    "# HISTORY Object which contains how the model learned\n",
    "\n",
    "# Training Values (Properties), Training Labels (Known Young's Moduli) \n",
    "history = model.fit(train_values, train_labels, batch_size = train_values.shape[0], \n",
    "                        epochs = EPOCHS, verbose = False, validation_split=0.1, callbacks=[mae_es, PrintEpNum()]) \n",
    "\n",
    "[loss_train, mae_train] = model.evaluate(train_values, train_labels, verbose=0)\n",
    "[loss_test, mae_test] = model.evaluate(test_values, test_labels, verbose=0)\n",
    "\n",
    "# Here is where we append the dropped variable, mae test, and train values\n",
    "none_removed = [\"none\", mae_train, mae_test]\n",
    "\n",
    "# Now, I want to create separate lists for the variable kept and the variables dropped\n",
    "vars_kept = list()\n",
    "vars_dropped = list()\n",
    "\n",
    "for i in range(0, len(my_values)):\n",
    "    vars_kept.append(my_values[i][0][0])\n",
    "    vars_dropped.append(my_values[i][0][1:])\n",
    "\n",
    "# counting variable\n",
    "i = 0\n",
    "\n",
    "# new list\n",
    "everything = list()\n",
    "# while loop to make this all one list\n",
    "while i < len(vars_kept):\n",
    "    everything.append([vars_kept[i], vars_dropped[i], my_values[i][1], my_values[i][2]])\n",
    "    i = i + 1\n",
    "# print(everything)\n",
    "\n",
    "# Finally, we should create a dataframe for these values\n",
    "labels = ['variable', 'variables_dropped','mae_train_value', 'mae_test_value']\n",
    "\n",
    "new_df = pd.DataFrame(everything , columns = labels)\n",
    "new_df = new_df.sort_values(by = 'mae_test_value')\n",
    "new_df.head(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
